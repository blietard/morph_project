{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'uralic/fin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sigmorphon2020/DEVELOPMENT-LANGUAGES/'+lang+'.trn', encoding='utf8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = [sent.split('\\t') for sent in data]\n",
    "    data.pop(-1)\n",
    "data = np.array(data,dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADJ;ACC;PL',\n",
       " 'ADJ;ACC;SG',\n",
       " 'ADJ;AT+ABL;PL',\n",
       " 'ADJ;AT+ABL;SG',\n",
       " 'ADJ;AT+ALL;PL',\n",
       " 'ADJ;AT+ALL;SG',\n",
       " 'ADJ;AT+ESS;PL',\n",
       " 'ADJ;AT+ESS;SG',\n",
       " 'ADJ;COM;PL',\n",
       " 'ADJ;FRML;PL',\n",
       " 'ADJ;FRML;SG',\n",
       " 'ADJ;GEADJ;PL',\n",
       " 'ADJ;GEADJ;SG',\n",
       " 'ADJ;IN+ABL;PL',\n",
       " 'ADJ;IN+ABL;SG',\n",
       " 'ADJ;IN+ALL;PL',\n",
       " 'ADJ;IN+ALL;SG',\n",
       " 'ADJ;IN+ESS;PL',\n",
       " 'ADJ;IN+ESS;SG',\n",
       " 'ADJ;INS;PL',\n",
       " 'ADJ;NOM;PL',\n",
       " 'ADJ;NOM;SG',\n",
       " 'ADJ;PRIV;PL',\n",
       " 'ADJ;PRIV;SG',\n",
       " 'ADJ;PRT;PL',\n",
       " 'ADJ;PRT;SG',\n",
       " 'ADJ;TRANS;PL',\n",
       " 'ADJ;TRANS;SG',\n",
       " 'N;ACC;PL',\n",
       " 'N;ACC;SG',\n",
       " 'N;AT+ABL;PL',\n",
       " 'N;AT+ABL;SG',\n",
       " 'N;AT+ALL;PL',\n",
       " 'N;AT+ALL;SG',\n",
       " 'N;AT+ESS;PL',\n",
       " 'N;AT+ESS;SG',\n",
       " 'N;COM;PL',\n",
       " 'N;FRML;PL',\n",
       " 'N;FRML;SG',\n",
       " 'N;GEN;PL',\n",
       " 'N;GEN;SG',\n",
       " 'N;IN+ABL;PL',\n",
       " 'N;IN+ABL;SG',\n",
       " 'N;IN+ALL;PL',\n",
       " 'N;IN+ALL;SG',\n",
       " 'N;IN+ESS;PL',\n",
       " 'N;IN+ESS;SG',\n",
       " 'N;INS;PL',\n",
       " 'N;NOM;PL',\n",
       " 'N;NOM;SG',\n",
       " 'N;PRIV;PL',\n",
       " 'N;PRIV;SG',\n",
       " 'N;PRT;PL',\n",
       " 'N;PRT;SG',\n",
       " 'N;TRANS;PL',\n",
       " 'N;TRANS;SG',\n",
       " 'V;COND;PL;1;POS;PRS;ACT',\n",
       " 'V;COND;PL;2;POS;PRS;ACT',\n",
       " 'V;COND;PL;3;POS;PRS;ACT',\n",
       " 'V;COND;POS;PRS;PASS',\n",
       " 'V;COND;SG;1;POS;PRS;ACT',\n",
       " 'V;COND;SG;2;POS;PRS;ACT',\n",
       " 'V;COND;SG;3;POS;PRS;ACT',\n",
       " 'V;IMP;PL;1;POS;PRS;ACT',\n",
       " 'V;IMP;PL;2;POS;PRS;ACT',\n",
       " 'V;IMP;PL;3;POS;PRS;ACT',\n",
       " 'V;IMP;POS;PRS;PASS',\n",
       " 'V;IMP;SG;2;POS;PRS;ACT',\n",
       " 'V;IMP;SG;3;POS;PRS;ACT',\n",
       " 'V;IND;PL;1;POS;PRS;ACT',\n",
       " 'V;IND;PL;1;POS;PST;ACT',\n",
       " 'V;IND;PL;2;POS;PRS;ACT',\n",
       " 'V;IND;PL;2;POS;PST;ACT',\n",
       " 'V;IND;PL;3;POS;PRS;ACT',\n",
       " 'V;IND;PL;3;POS;PST;ACT',\n",
       " 'V;IND;POS;PRS;PASS',\n",
       " 'V;IND;POS;PST;PASS',\n",
       " 'V;IND;SG;1;POS;PRS;ACT',\n",
       " 'V;IND;SG;1;POS;PST;ACT',\n",
       " 'V;IND;SG;2;POS;PRS;ACT',\n",
       " 'V;IND;SG;2;POS;PST;ACT',\n",
       " 'V;IND;SG;3;POS;PRS;ACT',\n",
       " 'V;IND;SG;3;POS;PST;ACT',\n",
       " 'V;NFIN',\n",
       " 'V;POT;PL;1;POS;PRS;ACT',\n",
       " 'V;POT;PL;2;POS;PRS;ACT',\n",
       " 'V;POT;PL;3;POS;PRS;ACT',\n",
       " 'V;POT;POS;PRS;PASS',\n",
       " 'V;POT;SG;1;POS;PRS;ACT',\n",
       " 'V;POT;SG;2;POS;PRS;ACT',\n",
       " 'V;POT;SG;3;POS;PRS;ACT']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data,columns=['Lemma','Forms','Attrs'],dtype='object')\n",
    "list(df.groupby('Attrs').groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoder():\n",
    "    '''\n",
    "    BPE algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ngrams  = {'UNK'}\n",
    "        self.word_series = None\n",
    "        self.most_freq_pair = None\n",
    "\n",
    "    def get_most_freq_pair(self):\n",
    "        pairs = defaultdict(int)\n",
    "        for form in self.word_series:\n",
    "            symbols = form.split()\n",
    "            for i in range(len(symbols) -1):\n",
    "                pairs[symbols[i],symbols[i+1]] += 1\n",
    "        return max(pairs, key=pairs.get)\n",
    "\n",
    "    def merge(self):\n",
    "        ab = ''.join(self.most_freq_pair)\n",
    "        # add 'ab' to the set of ngrams\n",
    "        self.ngrams.add(ab)\n",
    "        new_series = np.empty(len(self.word_series),dtype='object')\n",
    "        for i,form in enumerate(self.word_series):\n",
    "            # turn 'a b' to 'ab'\n",
    "            new_series[i] = form.replace(' '.join(self.most_freq_pair),ab )\n",
    "        self.word_series = new_series\n",
    "    \n",
    "    def fit(self,corpus,niter):\n",
    "        self.word_series = [' '.join(list(w)) for w in corpus]\n",
    "        for word in self.word_series:\n",
    "            s = set(word)\n",
    "            self.ngrams = self.ngrams | s\n",
    "\n",
    "        for _ in range(niter):\n",
    "            self.most_freq_pair = self.get_most_freq_pair()\n",
    "            self.merge()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramsExtractor():\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.train_stems = None\n",
    "        self.ntrains = 0\n",
    "        self.mapping = dict()\n",
    "    \n",
    "    def fit(train_set,stems = None):\n",
    "        if stems is None:\n",
    "            stemmer = Stemmer()\n",
    "            stemmer.fit(train_set)\n",
    "            raise NotImplementedError\n",
    "        else:\n",
    "            if len(stems)!= data.shape[0]:\n",
    "                raise ValueError('Length of stems list not matching dataset length.')\n",
    "            self.train_stems = pd.Series(stems)\n",
    "        \n",
    "        self.train_data = pd.DataFrame(train_set,columns=['Lemma','Forms','Attrs'],dtype='object')\n",
    "        self.ntrains = self.train_data.shape[0]\n",
    "        grouped = self.train_data.groupby('Attrs')\n",
    "        groups = grouped.groups.keys()\n",
    "        for group in groups:\n",
    "            \n",
    "            grouped.get_group(group)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ' 'UNK' 'a' 'e' 'erai' 'f' 'g' 'go' 'goû' 'goût' 'i' 'm' 'ma' 'man'\n",
      " 'mang' 'n' 'ni' 'o' 'p' 'r' 'ra' 'rai' 's' 't' 'u' 'û']\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder()\n",
    "bpe.fit([\"mangerai\",\"mangions\",\"goûterai\",\"goûta\",\"punirai\",\"finissais\", \"finiras\"],10)\n",
    "print(np.sort(list(bpe.ngrams)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "467c0426dd2210f1e0e2bd5481e6512c2fb39890cccfd8696cf90a79bf1f6bea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
