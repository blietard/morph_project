{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'germanic/eng'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../sigmorphon2020/DEVELOPMENT-LANGUAGES/'+lang+'.trn', encoding='utf8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = [sent.split('\\t') for sent in data]\n",
    "    data.pop(-1)\n",
    "data = np.array(data,dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "er\n"
     ]
    }
   ],
   "source": [
    "class BytePairEncoder():\n",
    "    '''\n",
    "    BPE algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ngrams  = {'UNK'}\n",
    "        self.word_series = None\n",
    "        self.ntrain = None\n",
    "        self.most_freq_pair = None\n",
    "\n",
    "    def get_most_freq_pair(self):\n",
    "        pairs = defaultdict(int)\n",
    "        for form in self.word_series:\n",
    "            symbols = form.split()\n",
    "            for i in range(len(symbols) -1):\n",
    "                pairs[symbols[i],symbols[i+1]] += 1\n",
    "        return max(pairs, key=pairs.get)\n",
    "\n",
    "    def merge(self):\n",
    "        ab = ''.join(self.most_freq_pair)\n",
    "        # add 'ab' to the set of ngrams\n",
    "        self.ngrams.add(ab)\n",
    "        new_series = np.empty(len(self.word_series),dtype='object')\n",
    "        for i,form in enumerate(self.word_series):\n",
    "            # turn 'a b' to 'ab'\n",
    "            new_series[i] = form.replace(' '.join(self.most_freq_pair),ab )\n",
    "        self.word_series = new_series\n",
    "    \n",
    "    def fit(self,corpus,niter):\n",
    "        self.word_series = [' '.join(list(w)) for w in corpus]\n",
    "        self.ntrain = len(self.word_series)\n",
    "        for word in self.word_series:\n",
    "            s = set(word)\n",
    "            self.ngrams = self.ngrams | s\n",
    "        for _ in range(niter):\n",
    "            self.most_freq_pair = self.get_most_freq_pair()\n",
    "            self.merge()\n",
    "\n",
    "    def get_frequencies(self):\n",
    "        ngrams_freqs = defaultdict(int)\n",
    "        for word in self.word_series:\n",
    "            ngrams_list = word.split()\n",
    "            for ngram in ngrams_list:\n",
    "                ngrams_freqs[ngram] += 1\n",
    "        return ngrams_freqs\n",
    "\n",
    "bpe = BytePairEncoder()\n",
    "bpe.fit(['tall','taller','fast','faster','further','far'],7)\n",
    "print(max(bpe.get_frequencies(),key=bpe.get_frequencies().get))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NgramsExtractor():\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.train_data = None\n",
    "        self.train_stems = None\n",
    "        self.ntrains = 0\n",
    "        self.mapping = dict()\n",
    "    \n",
    "    def fit(self,train_set,niter=10):\n",
    "        self.train_data = pd.DataFrame(train_set,columns=['Lemma','Forms','Attrs'],dtype='object')\n",
    "        self.ntrains = self.train_data.shape[0]\n",
    "        grouped = self.train_data.groupby('Attrs')\n",
    "        groups = grouped.groups.keys()\n",
    "        for group in groups:\n",
    "            df = grouped.get_group(group)\n",
    "            bpe = BytePairEncoder()\n",
    "            if df.shape[0]<2:\n",
    "                bpe.fit( list(df['Forms']) ,0)\n",
    "            else:\n",
    "                bpe.fit( list(df['Forms']) ,niter)\n",
    "            self.mapping[group] = (bpe,bpe.ntrain)\n",
    "\n",
    "    def get_ngrams_from_attrs(self,attrs,threshold=0.5):\n",
    "        try : \n",
    "            bpe,n = self.mapping[attrs]\n",
    "        except KeyError:\n",
    "            raise KeyError('Morphological attributes never encounters in training')\n",
    "        frequencies = bpe.get_frequencies()\n",
    "        ngrams_freqs = np.array(list(frequencies.items()),dtype='object')\n",
    "        mask = ngrams_freqs[:,1].astype('int32')>threshold*n\n",
    "        above_threshold = ngrams_freqs[mask][:,0]\n",
    "        return above_threshold\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'m': 3471, 'is': 1619, 'c': 5069, 'a': 7654, 'l': 6229, 'i': 8022, 'b': 2772, 'r': 4767, 'ated': 1540, 's': 5261, 'u': 4898, 't': 5821, 'n': 6888, 'ed': 11060, 'g': 2662, 're': 2060, 'e': 6122, 'f': 2332, 'o': 7699, 'd': 3817, 'w': 1334, '-': 436, 'p': 4255, 'z': 1559, 'h': 2908, 'red': 729, 'v': 1559, 'ere': 77, 'y': 792, 'ered': 678, 'ted': 1716, 'rer': 12, 'er': 1528, 'j': 240, 'k': 1368, 'q': 260, 'x': 400, 'ë': 17, 'é': 8, 'P': 17, 'S': 31, 'G': 8, 'A': 21, 'B': 22, 'O': 13, '*': 6, 'rere': 16, 'I': 19, 'T': 18, 'K': 2, 'æ': 38, \"'\": 25, 'M': 32, 'V': 4, 'L': 17, 'Z': 4, 'Q': 4, 'H': 8, 'D': 11, 'E': 10, '’': 1, 'œ': 8, 'R': 10, 'C': 20, 'J': 5, 'U': 3, 'ö': 2, 'Y': 3, ')': 1, 'X': 3, 'F': 11, 'ä': 2, '@': 1, 'ï': 1, 'N': 3, 'W': 3, '8': 1, '6': 1, '9': 1, '1': 2, '/': 1, 'ô': 1})\n"
     ]
    }
   ],
   "source": [
    "print(extractor.mapping['V;PST'][0].get_frequencies())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ed'], dtype=object)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = NgramsExtractor()\n",
    "extractor.fit(data,niter=6)\n",
    "extractor.get_ngrams_from_attrs('V;PST',threshold=0.60)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "467c0426dd2210f1e0e2bd5481e6512c2fb39890cccfd8696cf90a79bf1f6bea"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
