{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'austronesian/tgl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task0-data/DEVELOPMENT-LANGUAGES/'+lang+'.trn', encoding='utf8') as f:\n",
    "    data = f.read().split('\\n')\n",
    "    data = [sent.split('\\t') for sent in data]\n",
    "    data.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:5]\n",
    "for l in data:\n",
    "    if len(l) != 3:\n",
    "        print('problem: {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = dict()\n",
    "count = dict()\n",
    "for sent in data:\n",
    "    try :\n",
    "        count[sent[0]]+=1\n",
    "        dic[sent[0]].append( (sent[1],sent[2]) )\n",
    "    except KeyError:\n",
    "        try : \n",
    "            count[sent[0]] = 1\n",
    "            dic[sent[0]] =  [ (sent[1],sent[2]) ,]\n",
    "        except IndexError:\n",
    "            raise IndexError(str(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "liban\n",
      "lib nililibanan V;IPFV;PFOC\n",
      "lib lumiban V;PFV;AGFOC\n",
      "lib lumiliban V;IPFV;AGFOC\n"
     ]
    }
   ],
   "source": [
    "word = list(dic.keys())[2]\n",
    "print('liban')\n",
    "l = list(zip(['lib']*3,dic['liban']))\n",
    "for s, (f, a) in l:\n",
    "    print(s,f,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Forms</th>\n",
       "      <th>Attrs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>aabutin</td>\n",
       "      <td>V;PFOC;LGSPEC1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>abot</td>\n",
       "      <td>V;NFIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516</th>\n",
       "      <td>umaabot</td>\n",
       "      <td>V;IPFV;AGFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072</th>\n",
       "      <td>inabutan</td>\n",
       "      <td>V;PFV;PFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>umabot</td>\n",
       "      <td>V;PFV;AGFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>nag-aabot</td>\n",
       "      <td>V;IPFV;AGFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>inabot</td>\n",
       "      <td>V;PFV;PFOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1850</th>\n",
       "      <td>inaabutan</td>\n",
       "      <td>V;IPFV;PFOC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Forms           Attrs\n",
       "194     aabutin  V;PFOC;LGSPEC1\n",
       "195        abot          V;NFIN\n",
       "516     umaabot    V;IPFV;AGFOC\n",
       "1072   inabutan      V;PFV;PFOC\n",
       "1218     umabot     V;PFV;AGFOC\n",
       "1486  nag-aabot    V;IPFV;AGFOC\n",
       "1505     inabot      V;PFV;PFOC\n",
       "1850  inaabutan     V;IPFV;PFOC"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data,columns=['Lemma','Forms','Attrs'],dtype='object')\n",
    "grouped = df.groupby('Lemma')\n",
    "d = {}\n",
    "groups = grouped.groups.keys()\n",
    "#list(grouped.get_group('liban')['Forms'])\n",
    "for group in groups:\n",
    "    d[group] = grouped.get_group(group)[['Forms','Attrs']]\n",
    "d['abot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule 1 : the lemma determines the first the grammatical class (N,V,etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the stem (longest common substring) from the string array\n",
    "def findstem(arr):\n",
    "    accept = False\n",
    "    res = \"\"\n",
    "    s = arr[0] # Take first word from array as reference\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        for j in range(i + 1, len(s) + 1):\n",
    "\n",
    "            # generating all possible substrings of our reference string\n",
    "            stem = s[i:j]\n",
    "            k = 1\n",
    "            for k in range(1, len(arr)):\n",
    "                accept = True\n",
    "                if stem not in arr[k]:\n",
    "                    accept = False\n",
    "                    break\n",
    "\n",
    "            # If current substring is present in all strings and its length is greater than current result\n",
    "            if (accept and len(res) < len(stem)):\n",
    "                res = stem\n",
    "\n",
    "    return res\n",
    "\n",
    "def stemDict(dic):\n",
    "    stem_dic = dict()\n",
    "    for lemma in list(dic.keys()):\n",
    "        lemma_plus_form = [lemma] + [a[0] for a in dic[lemma]]\n",
    "        stem = findstem(lemma_plus_form)\n",
    "        start = lemma.index(stem)\n",
    "        end = len(lemma) - (start+len(stem))\n",
    "        pos = (start, end)\n",
    "\n",
    "        try :\n",
    "            stem_dic[pos].append(lemma)\n",
    "        except KeyError:\n",
    "            stem_dic[pos] = [lemma ,]\n",
    "    return stem_dic\n",
    "\n",
    "def stemDictToXY(dic):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for key in dic.keys():\n",
    "        X += dic[key]\n",
    "        Y += [key]*len(dic[key])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1, 0), 141), ((0, 2), 13), ((0, 0), 112), ((1, 2), 21), ((2, 0), 5), ((2, 2), 1), ((1, 1), 16), ((1, 3), 5), ((0, 4), 8), ((0, 1), 12), ((4, 0), 1), ((0, 3), 7), ((1, 6), 1), ((3, 1), 1)]\n"
     ]
    }
   ],
   "source": [
    "stem_dict = stemDict(dic)\n",
    "x, y = stemDictToXY(stem_dict)\n",
    "print([(key, len(stem_dict[key])) for key in stem_dict.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def customMetric(x, y):\n",
    "    return enchant.utils.levenshtein(x, y) / (len(x) + len(y))\n",
    "\n",
    "class Knn:\n",
    "    def __init__(self, n_neighbors):\n",
    "        self.k = n_neighbors\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.X = np.array(x)\n",
    "        self.y = np.array(y)\n",
    "     \n",
    "    def predict(self, test):\n",
    "        dists = np.zeros(len(self.X))\n",
    "        for i in range(len(self.X)):\n",
    "            dists[i] = customMetric(self.X[i], test)\n",
    "        nn_ids = dists.argsort()[:self.k]\n",
    "        return tuple(scipy.stats.mode(self.y[nn_ids])[0][0])\n",
    "\n",
    "knn = Knn(n_neighbors=1)\n",
    "knn.fit(x,y)\n",
    "y_hat = knn.predict('liban')\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5555555555555556, 'patay')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customMetric(x[0,0],'test'), x[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gramm_classes = dict()\n",
    "for lemma in dic.keys():\n",
    "    gramm_classes[lemma] = dict()\n",
    "    for form in dic[lemma]:\n",
    "        gc = form[1].split(';')[0]\n",
    "        try:\n",
    "            gramm_classes[lemma][gc]+=1\n",
    "        except KeyError:\n",
    "            gramm_classes[lemma][gc]=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceptions to Rule1 :  0\n",
      "Nb of lemma :  344\n",
      "% of failure :  0.0\n"
     ]
    }
   ],
   "source": [
    "#Number of exceptions\n",
    "gramm_class_except = [lemma for lemma in gramm_classes.keys() if len(gramm_classes[lemma])>1]\n",
    "print('Exceptions to Rule1 : ',len(gramm_class_except))\n",
    "print('Nb of lemma : ', len(dic))\n",
    "print('% of failure : ',round(len(gramm_class_except)/len(dic),4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('task0-data/DEVELOPMENT-LANGUAGES/'+lang+'.tst', encoding='utf8') as f:\n",
    "    test_data = f.read().split('\\n')\n",
    "    test_data = [sent.split('\\t') for sent in test_data]\n",
    "    test_data.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dic = dict()\n",
    "test_count = dict()\n",
    "for sent in test_data:\n",
    "    try :\n",
    "        test_count[sent[0]]+=1\n",
    "        test_dic[sent[0]].append( (sent[1]) )\n",
    "    except KeyError:\n",
    "        try : \n",
    "            test_count[sent[0]] = 1\n",
    "            test_dic[sent[0]] =  [ (sent[1]) ,]\n",
    "        except IndexError:\n",
    "            raise IndexError(str(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_train = 0\n",
    "train_lemmas_list = list(dic.keys())\n",
    "for lemma in list(test_dic.keys()):\n",
    "    if lemma in train_lemmas_list:\n",
    "        not_in_train +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(test_dic.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "266"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_in_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 ['AGFOC' 'IPFV' 'LGSPEC1' 'NFIN' 'PFOC' 'PFV' 'V']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_np = np.array(data)\n",
    "morpho_attribute_raw = data_np[:,2]\n",
    "morpho_attribute_splitted  = [attr.split(';') for attr in morpho_attribute_raw]       \n",
    "morpho_attribute_flat = np.array([item for sublist in morpho_attribute_splitted for item in sublist])\n",
    "\n",
    "list_morpho_attribute = np.unique(morpho_attribute_flat)\n",
    "print(len(list_morpho_attribute), list_morpho_attribute)\n",
    "\n",
    "# onehot_dict = {}\n",
    "# for i, m in enumerate(list_morpho_attribute):\n",
    "#     #vect = [0]*len(list_morpho_attribute)\n",
    "#     vect = np.zeros(len(list_morpho_attribute))\n",
    "#     vect[i] = 1\n",
    "#     onehot_dict[m] = vect\n",
    "    \n",
    "# onehot_dict"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "467c0426dd2210f1e0e2bd5481e6512c2fb39890cccfd8696cf90a79bf1f6bea"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
